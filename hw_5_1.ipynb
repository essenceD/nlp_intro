{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Домашнее задание 5\n",
        "\n",
        "Задание 1. Написать теггер на данных с русским языком \n",
        "проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
        "написать свой теггер как на занятии, попробовать разные векторайзеры добавить знание не только букв но и слов\n",
        "сравнить все реализованные методы, сделать выводы"
      ],
      "metadata": {
        "id": "Oy-ZMrbw24o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyconll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLzFafrq25J2",
        "outputId": "13fdbd87-e1b4-4194-b762-77c836e4a764"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('tagsets')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger\n",
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "import pyconll\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGYFUA8J3BSf",
        "outputId": "6c15ec38-f3b9-4940-c5ce-dca4c82b37c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset_ru\n",
        "\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxcyBrEI3D6_",
        "outputId": "d3eff3c4-2f25-4fd3-d22d-49e74010ca47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-12 15:03:47--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736581 (39M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  38.85M   196MB/s    in 0.2s    \n",
            "\n",
            "2022-12-12 15:03:48 (196 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’ saved [40736581/40736581]\n",
            "\n",
            "--2022-12-12 15:03:48--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42819832 (41M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  40.84M   196MB/s    in 0.2s    \n",
            "\n",
            "2022-12-12 15:03:49 (196 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’ saved [42819832/42819832]\n",
            "\n",
            "--2022-12-12 15:03:49--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32367510 (31M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  30.87M   171MB/s    in 0.2s    \n",
            "\n",
            "2022-12-12 15:03:50 (171 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’ saved [32367510/32367510]\n",
            "\n",
            "--2022-12-12 15:03:50--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14704579 (14M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  14.02M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-12 15:03:50 (123 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_train = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-a.conllu')\n",
        "full_train_b = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-b.conllu')\n",
        "full_train_c = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-c.conllu')\n",
        "\n",
        "# Общая обучающая выборка\n",
        "full_train.extend([*full_train_b, *full_train_c])\n",
        "\n",
        "full_test = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-dev.conllu')"
      ],
      "metadata": {
        "id": "b6PKKdFS3QdQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in full_test[:1]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ap4SEVM3UTg",
        "outputId": "c6e4e03a-1101-435f-c8c7-2991d7b1e52b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Алгоритм NOUN\n",
            ", PUNCT\n",
            "от ADP\n",
            "имени NOUN\n",
            "учёного NOUN\n",
            "аль PART\n",
            "- PUNCT\n",
            "Хорезми PROPN\n",
            ", PUNCT\n",
            "- PUNCT\n",
            "точный ADJ\n",
            "набор NOUN\n",
            "инструкций NOUN\n",
            ", PUNCT\n",
            "описывающих VERB\n",
            "порядок NOUN\n",
            "действий NOUN\n",
            "исполнителя NOUN\n",
            "для ADP\n",
            "достижения NOUN\n",
            "результата NOUN\n",
            "решения NOUN\n",
            "задачи NOUN\n",
            "за ADP\n",
            "конечное ADJ\n",
            "время NOUN\n",
            ". PUNCT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "metadata": {
        "id": "kL0nLXyC3WoX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(fdata_sent_test[50]), unigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "tdPgVJSz3YwU",
        "outputId": "fb13d0d9-9362-4d91-ca42-6294aab1781d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', 'PRON'),\n",
              " ('алгорифм', None),\n",
              " ('(', 'PUNCT'),\n",
              " ('кстати', 'ADV'),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', 'VERB'),\n",
              " ('написание', 'NOUN'),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', 'ADP'),\n",
              " ('фиту', None),\n",
              " (')', 'PUNCT'),\n",
              " ('производится', 'VERB'),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('арабского', 'ADJ'),\n",
              " ('слова', 'NOUN'),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', 'SCONJ'),\n",
              " ('есть', 'VERB'),\n",
              " ('корень', 'NOUN'),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8782863467673677"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(fdata_sent_test[50]), bigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "eBC3J3lK3ah9",
        "outputId": "86cb221c-cb5d-4f66-ac6d-49370e46c7fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', 'PRON'),\n",
              " ('алгорифм', None),\n",
              " ('(', 'PUNCT'),\n",
              " ('кстати', 'ADV'),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', None),\n",
              " ('написание', None),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', 'ADP'),\n",
              " ('фиту', None),\n",
              " (')', 'PUNCT'),\n",
              " ('производится', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('арабского', 'ADJ'),\n",
              " ('слова', 'NOUN'),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', 'SCONJ'),\n",
              " ('есть', 'VERB'),\n",
              " ('корень', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.7101308678950452"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(fdata_sent_test[50]), trigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "5mPsrh563caU",
        "outputId": "cb8d6259-dc87-4a05-b7bd-62a9e210e018"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', None),\n",
              " ('алгорифм', None),\n",
              " ('(', None),\n",
              " ('кстати', None),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', None),\n",
              " ('написание', None),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', None),\n",
              " ('фиту', None),\n",
              " (')', None),\n",
              " ('производится', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', None),\n",
              " ('арабского', None),\n",
              " ('слова', None),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', None),\n",
              " ('есть', None),\n",
              " ('корень', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4067191874470994"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим самые лучшие результаты показывает UnigramTagger, который не использует соседние слова для определения тэга слова.\n",
        "\n",
        "Попробуем создать комбинацию тэггеров, чтобы уменьшить количество нераспознаваемых слов, и посмотрим, как это повлияет на точность модели."
      ],
      "metadata": {
        "id": "dn0R4GTd3jrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "# В качестве бэкофф опции будем использовать тэг существительного\n",
        "backoff = DefaultTagger('NOUN') \n",
        "tag = backoff_tagger(fdata_train,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(fdata_test) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdj-tBgq3eFu",
        "outputId": "ea4469b3-261c-4ee9-8988-4206bc758500"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9119799466111075"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим благодаря этому подходу удалось добиться улучшения точности модели.\n",
        "\n",
        "Теперь построим модель тэггера на основе разных векторайзеров, которая будет фактически классифицировать каждое слово из датасета. Для этого для начала подготовим обучающий и тестовый наборы данных."
      ],
      "metadata": {
        "id": "yk8gt8aT3or2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "metadata": {
        "id": "TSpVdkxH3mI4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)\n",
        "test_enc_labels = le.transform(test_label)"
      ],
      "metadata": {
        "id": "6xTrG8zq3qwb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5BLKZXD3snA",
        "outputId": "bcb7cf19-c58d-4ecb-854c-bfba026af895"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
              "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'],\n",
              "      dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим несколько векторайзеров и обучим модель на их основе. Затем сравним получившиеся результаты."
      ],
      "metadata": {
        "id": "QV_qnaXY3xf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(2, 15), analyzer='char', n_features=65536)\n",
        "tvectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
        "cvectorizer = CountVectorizer(ngram_range=(2, 13), analyzer='char')"
      ],
      "metadata": {
        "id": "Lu-sYgS23vFv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xh_train = hvectorizer.fit_transform(train_tok)\n",
        "Xh_test = hvectorizer.transform(test_tok)\n",
        "\n",
        "Xt_train = tvectorizer.fit_transform(train_tok)\n",
        "Xt_test = tvectorizer.transform(test_tok)\n",
        "\n",
        "Xc_train = cvectorizer.fit_transform(train_tok)\n",
        "Xc_test = cvectorizer.transform(test_tok)"
      ],
      "metadata": {
        "id": "KCOhKKwh3zKE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xh_train, train_enc_labels)\n",
        "pred = lr.predict(Xh_test)\n",
        "print(f'Accuracy для модели тэггера на основе HashingVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP50jj-A31dO",
        "outputId": "65de6a3c-a113-4220-d5d2-00f42923a62c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе HashingVectorizer - 0.8609.\n",
            "CPU times: user 23min 45s, sys: 2min 54s, total: 26min 39s\n",
            "Wall time: 23min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xt_train, train_enc_labels)\n",
        "pred = lr.predict(Xt_test)\n",
        "print(f'Accuracy для модели тэггера на основе TfidfVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8CY8J1G33-K",
        "outputId": "78923639-6f84-4a91-f5d7-98a751633ea7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе TfidfVectorizer - 0.7563.\n",
            "CPU times: user 16min 49s, sys: 3min 6s, total: 19min 55s\n",
            "Wall time: 16min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xc_train, train_enc_labels)\n",
        "pred = lr.predict(Xc_test)\n",
        "print(f'Accuracy для модели тэггера на основе CountVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xtNKYwa36He",
        "outputId": "b48056cd-a36c-4935-e298-272f5e13a94a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе CountVectorizer - 0.8695.\n",
            "CPU times: user 57min 59s, sys: 5min 46s, total: 1h 3min 46s\n",
            "Wall time: 53min 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим наилучшая точность получилась среди векторайзеров получилась для модели CountVectorizer на основе букв. Но учитывая затраты времени и небольшрй прирост в точности, пальму первенства отдадим хэш-векторайзеру."
      ],
      "metadata": {
        "id": "zGNP4dOu3_zs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lITQvy3S373W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}